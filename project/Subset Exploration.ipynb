{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "What we did in this notebook is selecting a small part of the amazon dataset and use it to test the methods we implemented.\n",
    "\n",
    "We tried to write code which makes use of Spark SQL, however due to the limitated competences of certain members of the group we weren't always able to provide efficient code. This issue will be resolved in Milestone 3.\n",
    "\n",
    "Please also note that due to time constraints and cluster overload we weren't able to properly test our methods on the cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import/Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import findspark\n",
    "os.environ['SPARK_HOME'] = '/opt/apache-spark'\n",
    "# os.environ['SPARK_HOME'] = '/usr/local/Cellar/apache-spark/2.2.0/libexec/'\n",
    "findspark.init()\n",
    "from pyspark import SQLContext, SparkContext, SparkConf\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.mllib.stat import Statistics as S\n",
    "import json\n",
    "import numpy as np\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import *\n",
    "from pyspark_dist_explore import hist\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def correlation(df, col1, col2, method='pearson'):\n",
    "    rdd1 = df.select(col1).rdd.map(lambda x: x[0])\n",
    "    rdd2 = df.select(col2).rdd.map(lambda x: x[0])\n",
    "    return S.corr(rdd1, rdd2, method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creation of a Spark contest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "memory = '6g'\n",
    "pyspark_submit_args = ' --driver-memory ' + memory + ' pyspark-shell'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = pyspark_submit_args\n",
    "\n",
    "sc = SparkContext()\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the data from the json file. The data is a small subset containing only items from the following categories:\n",
    "\"Baby\", \"Automotive\" and \"Grocery and Gourmet Food\". We will use this dataset on our local machine to qualitatively manipulate the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-784e6f35567d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# df_review = sqlContext.read.json(\"reviews_auto_baby_grocery.json\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_review\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msqlContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"datasets/data.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf_review\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data.parquet\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/apache-spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mjson\u001b[0;34m(self, path, schema, primitivesAsString, prefersDecimal, allowComments, allowUnquotedFieldNames, allowSingleQuotes, allowNumericLeadingZero, allowBackslashEscapingAnyCharacter, mode, columnNameOfCorruptRecord, dateFormat, timestampFormat, multiLine)\u001b[0m\n\u001b[1;32m    247\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/apache-spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1129\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1131\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1133\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/opt/apache-spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    881\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 883\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    884\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/apache-spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1026\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1028\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1029\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/idp/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df_review = sqlContext.read.json(\"reviews_auto_baby_grocery.json\")\n",
    "# df_review = sqlContext.read.json(\"datasets/data.json\")\n",
    "df_review.write.parquet(\"data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_review = sqlContext.read.json(\"reviews_auto_baby_grocery.json\")\n",
    "df_review = sqlContext.read.parquet(\"data.parquet\")\n",
    "\n",
    "num_reviews = df_review.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we load the product's metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# meta_df = sqlContext.read.json(\"meta_auto_baby_grocery.json\")\n",
    "meta_df = sqlContext.read.parquet(\"metadata.parquet\")\n",
    "#meta_df = sqlContext.read.json(\"datasets/metadata.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup for parquet file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We flatten the meta dataframe\n",
    "def flatten(salesRank):\n",
    "    if salesRank == None:\n",
    "        return None, None\n",
    "    else:\n",
    "        for category, rank in salesRank.asDict().items():\n",
    "            if rank != None:\n",
    "                return (category, rank)\n",
    "    return None, None\n",
    "flatten = udf(flatten, StructType([StructField(\"category\", StringType()), StructField(\"rank\", IntegerType())]))\n",
    "df_flattened = meta_df.withColumn(\"salesRank\", flatten(\"salesRank\"))\n",
    "df_flattened = df_flattened.drop(\"_corrupt_record\")\n",
    "df_flattened = df_flattened.select(list(set(df_flattened.schema.names) - set([\"salesRank\", \"related\"]))+ [\"salesRank.*\", \"related.*\"])\n",
    "df_flattened.write.parquet(\"metadata.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[description: string, categories: array<array<string>>, title: string, imUrl: string, asin: string, price: double, brand: string, category: string, rank: int, also_bought: array<string>, also_viewed: array<string>, bought_together: array<string>, buy_after_viewing: array<string>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is 0 article with not exactly one category for a rank\n"
     ]
    }
   ],
   "source": [
    "# No need to run it, already checked...\n",
    "\n",
    "# We check that there is no metadata with two ranks\n",
    "def not_one_or_none(salesRank):\n",
    "    count = 0\n",
    "    # If no rank, then false\n",
    "    if salesRank == None:\n",
    "        return 0\n",
    "    for category, rank in salesRank.asDict().items():\n",
    "        if rank != None:\n",
    "            count += 1\n",
    "    return int(count != 1 and count != 0)\n",
    "not_one_or_none = udf(not_one_or_none, IntegerType())\n",
    "not_one_or_none = meta_df.withColumn(\"exactly_one\", not_one_or_none(\"salesRank\")).agg(F.sum(\"exactly_one\").alias(\"sum\")).head()[0]\n",
    "print(\"There is %.d article with not exactly one category for a rank\"%not_one_or_none)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We filter the salesRank to have only ranked products. There is two possiblity, either the salesRank is completely none, either the categories are all None. we take care of both cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6151598"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_meta_df = df_flattened.dropna(subset=['rank', \"category\"])\n",
    "filtered_meta_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9430088"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_flattened.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Question 1: review quantity vs quality "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we explore some metrics related to reviews: a quantitative one - the number of words per review - and a qualitative one - the helpfulness of a review. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word count\n",
    "Here we count the number of word in a certain review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordCount = F.udf(lambda text: len(text.split(\" \")), T.IntegerType())\n",
    "\n",
    "df = df_review.withColumn('wordcount', wordCount(df_review[\"reviewText\"]))\n",
    "df.select(\"wordcount\").describe().show()\n",
    "overall = df.select(\"overall\").rdd.map(lambda x: x[0])\n",
    "wordcount = df.select(\"wordcount\").rdd.map(lambda x: x[0])\n",
    "pearson_corr = correlation(df, \"overall\", \"wordcount\", method='pearson')\n",
    "spearman_corr = correlation(df, \"overall\", \"wordcount\", method='spearman')\n",
    "print(\"The pearson correlation is %.3f and the spearman is %.3f\"%(pearson_corr, spearman_corr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(8, 5)\n",
    "hist(ax, df.select(\"wordcount\"), log=True);\n",
    "pearson_corr = correlation(df.filter(df.wordcount > 1000), \"overall\", \"wordcount\", method='pearson')\n",
    "spearman_corr = correlation(df.filter(df.wordcount > 1000), \"overall\", \"wordcount\", method='spearman')\n",
    "print(\"The pearson correlation is %.3f and the spearman is %.3f\"%(pearson_corr, spearman_corr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpfulness\n",
    "We get the \"helpfulness\" of a review by computing the ratio of people that found a review helpful and those that voted. If we have no data, we simply put -1 as a ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_review = df_review.withColumn(\"helpfulness\", F.when(df_review.helpful.getField(1) > 0, df_review.helpful.getField(0)/df_review.helpful.getField(1)).otherwise(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_review.filter(df_review[\"helpfulness\"] >= 0.0).select(\"helpfulness\").describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seems to have some discrepancies amongst the ratio, since the max is at 2.0. We will check that it does not occur too often and may drop the data in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_review.filter(df_review[\"helpfulness\"] > 1.0).select(\"helpful\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since only two reviews have problems, we will simply ignore them.\n",
    "\n",
    "We then look at some distribution of helpfulness and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(8, 5)\n",
    "hist(ax, df_review.filter((df_review[\"helpfulness\"] >= 0.0) & (df_review[\"helpfulness\"] <= 1.0)).select(\"helpfulness\"));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(8, 5)\n",
    "hist(ax, df_review.filter((df_review[\"helpfulness\"] >= 0.0) & (df_review[\"helpfulness\"] <= 1.0)).select(\"overall\"));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correlation(df_review.filter((df_review[\"helpfulness\"] >= 0.0) & (df_review[\"helpfulness\"] <= 1.0)), \"overall\", \"helpfulness\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With only these three metrics we aren't able to draw any meaningful conclusion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we compute the correlation between a product ranking and -respectively - the number of reviews it has, the average of the reviews helpfulness, the average of the reviews ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_df = filtered_meta_df[['asin', 'unique_rank']]\n",
    "\n",
    "joined_df = filtered_meta_df.join(df_review, 'asin')\n",
    "grouped_df = joined_df.groupBy('asin')\n",
    "\n",
    "review_count_df = grouped_df.count()\n",
    "mean_helpfulness_df = grouped_df.mean('helpfulness', 'overall')\n",
    "output_df = output_df.join(review_count_df, 'asin').join(mean_helpfulness_df, 'asin')\n",
    "rank_count_correlation = output_df.corr('unique_rank', 'count', method='pearson')\n",
    "rank_helpfulness_correlation = output_df.corr('unique_rank', 'avg(helpfulness)', method='pearson')\n",
    "rank_overall_correlation = output_df.corr('unique_rank', 'avg(overall)', method='pearson')\n",
    "print(rank_count_correlation, rank_helpfulness_correlation, rank_overall_correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: influence of reviewer personality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the count of review of distinct article per reviewer and filter those that have less than 5 review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_count = df_review.groupBy(\"reviewerID\").agg(F.countDistinct(\"asin\").alias('count')).filter(\"`count` >= 5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get all reviews of reviewers with more than 5 articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_filtered = df_review.join(df_count, on='reviewerID').drop('count')\n",
    "num_five_reviews = df_filtered.count()\n",
    "print(\"We have %.d, i.e. %.3f%% of review who belongs to reviewer with 5 or more reviews\"%(num_five_reviews, num_five_reviews/num_reviews))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the average grade for each reviewer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "average_std_score = df_filtered.groupBy(\"reviewerID\").agg(F.avg(\"overall\").alias(\"mean\"), F.stddev(\"overall\").alias(\"std\"))\n",
    "average_std_score.cache().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some statistic concerning the average grade for each reviewer.\n",
    "\n",
    "The first row contains the total number of reviewer considered.\n",
    "\n",
    "The second row contains the mean of the reviewers' averages grades (left) and the standard deviation of said mean (right).\n",
    "\n",
    "The third row contains the mean of the reviewers' standard deviations (left) and the standard deviation of said mean (right).\n",
    "\n",
    "The fourth row contains the mean of the reviewer's lower grades (left) and the standard deviation of said mean (right).\n",
    "\n",
    "The fourth row contains the mean of the reviewer's higher grades (left) and the standard deviation of said mean (right)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "average_std_score.select(\"mean\", \"std\").describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then plot the average grade for each reviewer through histograms, with the grade on the x-axys and the number of reviewers who have such mean ranking on the y-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(8, 5)\n",
    "hist(ax, average_std_score.select(\"mean\"));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be noted that reviewers tend to give high ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then plot the average standard deviation for each reviewer through histograms, with the grade on the x-axys and the number of reviewers who have such average standard deviation on the y-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(8, 5)\n",
    "hist(ax, average_std_score.select(\"std\"));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be noted that the majority of reviewers tend to have either a very small standard deviation (0-0.25) or a standard deviation between 0.5 and 1.5. The number of people with an higher standard deviation are sensibily less."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we look at the mean of the ratings given by reviewer with small (<0.25) standard deviation.\n",
    "We also look at what percentage of reviewer they correspond."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"There is %.2f%% of reviewer with std < 0.25\"%(average_std_score.filter(average_std_score.std < 0.25).count()/average_std_score.count()*100))\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(8, 5)\n",
    "hist(ax, average_std_score.filter(average_std_score.std < 0.25).select(\"mean\"));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be noted that people with a small standard deviation, i.e. people who tend to always give the same rating, tend to give only very high rating (>4.5)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion, we have hints that there's a certain bias related to reviewers' personality and habits. \n",
    "The fact that the mean of the average reviewer rating is so high for example could make us think that people tend to rate the product they're satisfied with and not those they're unsastisfied with.\n",
    "The fact that the reviewers with little standard deviation tend to give high ratings also speaks in that direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3: Brand fidelity/hate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the metadata datas and only keep the article number and the brand that are not null, since we are interested in brands.\n",
    "\n",
    "We then join the dataframe of reviews with the metadata and display some stats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_meta = sqlContext.read.json(\"meta_auto_baby_grocery.json\")\n",
    "df_meta_brand = df_meta.select(\"asin\", \"brand\").dropna()\n",
    "\n",
    "df_with_brand = df_filtered.join(df_meta_brand, on='asin')\n",
    "num_reviews_with_brand = df_with_brand.count()\n",
    "\n",
    "print(\"We have %.d, which represents %.3f%% of review with a brand and who belongs to reviewer with 5 or more reviews\"%(num_reviews_with_brand, num_reviews_with_brand/num_reviews))\n",
    "print(\"We have %.3f%% of review amongst those who belongs to reviewer with 5 or more reviews that have a brand \"%(num_reviews_with_brand/num_five_reviews))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that once again we loose a lot of reviews with this approach, but it is still an interesting number to work with.\n",
    "\n",
    "Then we group by brand and reviewer, compute some stats and filter out all reviewers that have less than 5 reviews for a specific brand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_brand_reviewer = df_with_brand.groupBy([\"brand\", \"reviewerID\"]).agg(F.count(\"overall\").alias(\"count\"), F.avg(\"overall\").alias(\"mean\"), F.stddev(\"overall\").alias(\"std\")).filter(\"`count` >= 5\")\n",
    "brand_reviewers_count = df_brand_reviewer.cache().count()\n",
    "print(\"We have %.d brand/reviewers tuples\"%(brand_reviewers_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then show some stats about the mean and std."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_brand_reviewer.select(\"count\", \"mean\", \"std\").describe().show()\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(8, 5)\n",
    "hist(ax, df_brand_reviewer.select(\"mean\"));\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(8, 5)\n",
    "hist(ax, df_brand_reviewer.select(\"std\"));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By comparing qualitatively (we will do a more precise analysis in milestone 3) those graphs with the one from Question 2, we can see that the ratings seems to be having less deviation, showing a tendency to love/hate toward a brand.\n",
    "\n",
    "We try to see if there is an obvious correlation between number of reviews for a brand and the mean rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pearson_corr = correlation(df_brand_reviewer, \"count\", \"mean\", method='pearson')\n",
    "spearman_corr = correlation(df_brand_reviewer, \"count\", \"mean\", method='spearman')\n",
    "print(\"The pearson correlation is %.3f and the spearman is %.3f\"%(pearson_corr, spearman_corr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first sight, we cannot see an obvious relationship, however, we will do a more refined analysis in the next milestone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the total number of products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_products = meta_df.count()\n",
    "total_products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the ratio between ranked and total products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ranked_products = filtered_meta_df.count()\n",
    "ranked_products\n",
    "ratio = ranked_products/total_products\n",
    "print(\"ranked_products: \" + str(ranked_products) + \" total products: \" + str(total_products) + \" ratio: \" + str(ratio))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can notice that in this restricted dataset less than one third of the products is ranked, a fact that makes our rank-based analysis less representative. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now check that there is exactly one category that has a sales rank."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we obtain the names of the ranking categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rank_categories = filtered_meta_df.head().salesRank.asDict().keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4: influence of 'also bought' feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll begin by focus on the 'bought together' feature and not on the 'also bought' feature right away.\n",
    "This both because the former is more relevant to the question and because it scales better: 'also bought' tends to be a very long list of products while 'bought together' is usually much shorter and easier to analyze."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to answer question 4, it is possible to use the following approach:\n",
    "1. Find the best ranked products for each category.\n",
    "2. look at the rank of the products that were bought together with them.\n",
    "3. draw  qualitative conclusions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we select the interesting columns of the metadata dataframe, flattening the hierarchical datastructure in the process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we define a function that allows to find in which category a product has been ranked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(asin='B0000D94P1', unique_rank=118616, paired_product='B0000D94PL', asin='B0000D94PL', unique_rank=129998, paired_product='B0000D94P1')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#same analysis for bought_together!\n",
    "q4_meta_df = filtered_meta_df.select(\"asin\", \"related.*\", \"unique_rank\")\n",
    "q4_meta_df = q4_meta_df.select(\"asin\", \"unique_rank\", F.explode(\"bought_together\").alias(\"paired_product\")).dropna(how='any')\n",
    "q4_joined_meta_df = q4_meta_df.alias(\"df1\").join(q4_meta_df.alias(\"df2\"), F.col(\"df1.paired_product\") == F.col(\"df2.asin\"), \"inner\")\n",
    "q4_joined_meta_df.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be noted that products with very different rankings were bought together.\n",
    "This means that question 4 might indeed make sense, there might be \"good\" product that improve the sales of \"bad\" products. As of now \"good\" means \"popular\" and bad means \"unpopolar\", but the definition might change when we delve even further into the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5: rank/review ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The approach for question 5 is the following:\n",
    "1. For each rank category, take the 5 best ranked products\n",
    "2. look in the review table and see how many review were made for each product\n",
    "3. Print the values and try to qualitatevely interpet. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we apply the described pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for category in rank_categories:\n",
    "    category_df = filtered_meta_df.select(category, \"asin\").dropna().sort(category, ascending = False)\n",
    "    best_products = category_df.take(5)\n",
    "    for index, best_product in enumerate(best_products):\n",
    "        productid = best_product.asin\n",
    "        rank = best_product[category]\n",
    "        number_reviews = df_review.filter(review_df.asin == productid).count()\n",
    "        print(\"There are \" + str(number_reviews) + \" reviews for the product ranked \" + str(rank) + \" in the category \" + category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's doesn't seem to be any correlation between the number of reviews associated to a product and its sales ranking according to these results, which don't hint at a linear relationship between number of reviews and sales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6: product categorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I start by filtering the metadata to drop the uncategorized products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filtered_meta_df = meta_df.dropna(subset=['categories'])\n",
    "filtered_meta_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then I proceed to analyze the products.\n",
    "The approach is the following:\n",
    "1. I create a dictionary to map each category to the number of reviews written for that category.\n",
    "2. for each product, I extract the categories it is part of.\n",
    "3. I compute the number of reviews for that product and I update the values associated to the product categories in the dictionary.\n",
    "\n",
    "I limit the analysis to 10 products on my local machine due to the heavy computational cost, however on the cluster we'll analyze all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "category_dict = {}\n",
    "#NOTE: the commented line is the one to be used in the cluster.\n",
    "#for product in filtered_meta_df.collect():\n",
    "for product in filtered_meta_df.take(10):\n",
    "    productid = product.asin\n",
    "    categories = product.categories[0]\n",
    "    for category in categories:\n",
    "        number_reviews = review_df.filter(review_df.asin == productid).count()\n",
    "        old_value = category_dict.get(category, 0)\n",
    "        category_dict[category] = old_value + number_reviews\n",
    "\n",
    "print(str(category_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This small and potentially unrepresentative dataset shows there's a wide difference in the number of reviews between categories. To say more we need the results from the cluster, which we couldn't obtain due to time constraint and cluster overload."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# TO DO FOR MILESTONE 3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find metrics allowing to draw meaningful conclusions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better quantify the bias of the reviewer by analysing other aspects "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further delve into the data to produce a coherent conclusion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further delve into the data and formally describe the relation between the 'also bought' and 'bought together' features as well as the meaning of a \"good\" and \"bad\" product, which as of now is associated to popularity. \n",
    "Also substitute the print based approach with a more complex one.\n",
    "\n",
    "Draw formally correct conclusion with regard to the influence of the 'also_bought' feature on sales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm the conclusion by working on the whole dataset through the cluster and substitute the print based approach with a more complex one. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm the conclusion by working on the whole dataset through the cluster and substitute the dictionary based approach with a more complex one. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refine analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eventually find new question related to reviews and answer them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "produce a clear description of the characteristic of an amazon review: how can review be in interpreted in the complex system that is Amazon?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sum up the results of the research in a 4-page report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Poster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "design a poster to present the results of our research."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
